{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab2b191",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import send_file\n",
    "\n",
    "return send_file(\"testdata.xlsx\", as_attachment=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc36bd2",
   "metadata": {},
   "source": [
    "  #### Banking Knowledge for this project\n",
    "\n",
    "  Liability (Udhaar/Loan)\n",
    "\n",
    "- **Definition:**  \n",
    "  - Liability ka matlab hota hai koi financial zimmedaari ya udhaar jo kisi vyakti ya organization ko kisi doosre party ko chukana hota hai. Ye wo cheez hai jo tumhe future mein repay karni hoti hai.\n",
    "\n",
    "- **Customer Liabilty Example**\n",
    "  - Housing Loan\n",
    "  - Credit Card\n",
    "  - Car Loan\n",
    "  - Education Loan\n",
    "\n",
    "- **Banking Liabilty Example**\n",
    "  - Current Account\n",
    "  - Fixed Deposit\n",
    "  - Savings Accoount\n",
    "  - Recurring Deposit   \n",
    "\n",
    "---\n",
    "\n",
    "   Types of Liabilities\n",
    "\n",
    "- **Short-Term Liabilities (Chote Samay k liye Udhaari)**  \n",
    "  Ek saal ke andar repay hone wali zimmedaari  \n",
    "  - Examples: Credit card bill, electricity bill, short-term loan\n",
    "\n",
    "- **Long-Term Liabilities (Lambe Samay k liye Udhaari)**  \n",
    "  Ek saal ke baad repay hone wali zimmedaari  \n",
    "  - Examples: Home loan, car loan, student loan\n",
    "\n",
    "---\n",
    "\n",
    "    Banking Context\n",
    "\n",
    "- **Asset (Sampati/Kiss Cheez k liye Udhar Le rahien hain):**  \n",
    "  Jo cheez tumhare paas hai ya tum kharidna chahte ho (e.g., ghar, car)\n",
    "\n",
    "- **Liability (Loan/Udhaar):**  \n",
    "  Jo paisa tumne udhaar liya hai us asset ko kharidne ke liye\n",
    "\n",
    "- **Example:**  \n",
    "  - Agar kisi ne ‚Çπ10 lakh ka home loan liya hai, toh woh ‚Çπ10 lakh uski liability hai.\n",
    "\n",
    "---\n",
    "\n",
    "   Balance Sheet View\n",
    "- **Table to understand**\n",
    "| Category     | Example               |\n",
    "|--------------|----------------------|\n",
    "| Assets       | House, Cash, Stocks  |\n",
    "| Liabilities  | Home Loan, Credit Card Debt |\n",
    "\n",
    "---\n",
    "\n",
    "   Formula\n",
    "\n",
    "**Net Worth = Assets ‚Äì Liabilities**\n",
    "\n",
    "---\n",
    "  NPA(Non-Performing Asset)\n",
    "\n",
    "**NPA = Loan that is defaulted**\n",
    "\n",
    "- NPA padhne se pahle apne ko ye 4 terms pahle padhne padenge\n",
    "  - 1. Distributed Amount  = Loan Amount jo customer ko diya gaya hai usko hamm Distributed Amount kehte hain\n",
    "  - 2. OSP (Outstanding Principle)\n",
    "    - Maanlo Maine 1 Lakh Rupay Loan Liya\n",
    "    - Per Month 8000 ki EMI parr\n",
    "    - 40000 Hajar Maine Loan Repay Kar diya\n",
    "    - 60000 Bakaya = Balance = OSP(Outstanding Principle)\n",
    "    - OSP Should be zero at the end of the loan cycle  \n",
    "  - 3. DPD(Days Per Due)\n",
    "    - Maanlo  Mera EMI repayment ka date 10 October tha or usko mien 14 October ko Pay Karta hun to\n",
    "    - DPD = 4\n",
    "    - DPD ideally zero hona chahiye\n",
    "    - **Defaulted** DPD > 0\n",
    "  - 4. PAR (Portfolio At Risk)\n",
    "    - OSP when DPD>0\n",
    "\n",
    "**NPA**\n",
    "- Loan account when DPD > 90   \n",
    "- Then it is called NPA account    \n",
    "---\n",
    "<br>\n",
    "\n",
    "**Credit Risk Types in Banking**\n",
    "- DPD (Zero) = NDA(Non Deliquenent Account)\n",
    "- DPD (1 to 30) = SMA1 (Standard Monitoring Account)\n",
    "- DPD (31 to 60) = SMA2 (Standard Monitoring Account)\n",
    "- DPD (61 to 90) = SMA3 (Standard Monitoring Account)\n",
    "- DPD (91 t0 180) = NPA\n",
    "- DPD (> 180) = Written-Off (Loan which is not present) Usko Data se hata diya jayega or aisa kyun kiya jaata hai taaki bank ka **Loan Portfolio Quality** bank ka better banne\n",
    "\n",
    "**NPA improve  = Loan Portfolio Quality of the bank will be better = Market Sentiment is good  = Stock Price will improve**\n",
    "\n",
    "**NPA Types**\n",
    "- GNPA = Gross NPA (3-5 %) OSP Default hai\n",
    "- NNPA  = Net NPA (0.01 to 0.06) Provisioning Ammount\n",
    "\n",
    "Bank Quality access , to GNPA Value dekho jitni kam GNPA value utna acha Bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9879e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.stats import chi2_contingency,f_oneway\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, precision_recall_fscore_support\n",
    "import warnings\n",
    "import os\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4f95ec",
   "metadata": {},
   "source": [
    "  üìä Internal vs CIBIL (External) Dataset in Credit Risk Modelling\n",
    "\n",
    "Credit risk modelling ke liye banks aur NBFCs do major data sources use karte hain:  \n",
    "**Internal dataset** (apne customers ka data) aur **External dataset** (CIBIL ya kisi credit bureau ka data).  \n",
    "Dono ka role alag hota hai aur dono ko combine karke model ki accuracy badhayi ja sakti hai.\n",
    "\n",
    "   üè¶ Internal Dataset (Bank ka khud ka data)\n",
    "\n",
    "- **Source:**  \n",
    "  Bank ya NBFC apne customers se jo bhi loan, repayment, default, transaction history collect karta hai, woh internal dataset hota hai.\n",
    "\n",
    "- **Specificity:**  \n",
    "  Ye data institution-specific hota hai. Har bank sirf apne borrowers ka detailed behaviour track karta hai.\n",
    "\n",
    "- **Use Case:**  \n",
    "  - Scorecard development  \n",
    "  - Probability of default (PD) estimation  \n",
    "  - Loan pricing aur account management  \n",
    "  - Personalized offerings\n",
    "\n",
    "- **Customization:**  \n",
    "  Bank apne business logic, risk appetite aur policies ke hisaab se data ko customize karta hai.\n",
    "\n",
    "   üåê CIBIL / External Dataset (Credit Bureau Data)\n",
    "\n",
    "- **Source:**  \n",
    "  CIBIL, Equifax, Experian jaise bureaus multiple banks/NBFCs se data collect karke centralized repository mein rakhte hain.\n",
    "\n",
    "- **Standardization:**  \n",
    "  Ye data ek uniform format mein hota hai, jisme sabhi institutions common reporting guidelines follow karte hain.\n",
    "\n",
    "- **Use Case:**  \n",
    "  - Applicant ka market-wide credit behaviour check karna  \n",
    "  - Multi-institution loan history aur repayment pattern evaluate karna  \n",
    "  - Fraud detection aur cross-bank defaults identify karna\n",
    "\n",
    "- **Objectivity:**  \n",
    "  External dataset unbiased hota hai kyunki ye multiple sources se aata hai. Isse borrower ki overall creditworthiness samajh mein aati hai.\n",
    "\n",
    "   üìã Comparison Table\n",
    "\n",
    "| Feature                  | Internal Dataset                     | CIBIL (External) Dataset                |\n",
    "|--------------------------|--------------------------------------|-----------------------------------------|\n",
    "| **Source**               | Bank ka khud ka customer data        | Credit bureau se consolidated data      |\n",
    "| **Customization**        | Bank-specific scoring, tailor-made  | Industry-standard, uniform structure    |\n",
    "| **Coverage**             | Sirf apne customers                  | Har individual across institutions      |\n",
    "| **Use**                  | Internal decision-making             | Applicant evaluation by any institution |\n",
    "| **Update Frequency**     | Real-time / near real-time           | Monthly reporting, lag possible         |\n",
    "| **Risk Evaluation**      | Specific behaviour with that bank    | Overall credit health across market     |\n",
    "\n",
    "   üîç Credit Risk Modelling Mein Role\n",
    "\n",
    "- **Internal Data:**  \n",
    "  Loan pricing, customer segmentation, account-level risk management ke liye kaam aata hai.\n",
    "\n",
    "- **External Data:**  \n",
    "  Broader credit behaviour, fraud detection aur multi-loan exposure evaluate karne mein madad karta hai.\n",
    "\n",
    "- **Dual Rating Approach:**  \n",
    "  Dono datasets ko combine karke hybrid models banaye jaate hain jisse early risk detection aur model robustness improve hoti hai.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fbc98b",
   "metadata": {},
   "source": [
    "To Yahan parr hamm 2 dataset use karne waale hain Internal and External dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d1a8e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "internalData = pd.read_excel(\"/home/ankitmaan/Credit Risk Modelling/Datasets/case_study1.xlsx\")\n",
    "externalData = pd.read_excel(\"/home/ankitmaan/Credit Risk Modelling/Datasets/case_study2.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29aae0d1",
   "metadata": {},
   "source": [
    "Copy Bana Rahien hain taaki original data pe koi asar naa pade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc9f6426",
   "metadata": {},
   "outputs": [],
   "source": [
    "internal = internalData.copy()\n",
    "external = externalData.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1fdb71",
   "metadata": {},
   "source": [
    "Removing null values from Age_Oldest_TL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20c37ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "internal = internal.loc[internal['Age_Oldest_TL'] != -99999]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53541ffd",
   "metadata": {},
   "source": [
    " Vo columns ko hamne dataset se hatane k liye select kiya hai jismien null values 10000 se jyada thi  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e456da30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PROSPECTID</th>\n",
       "      <th>time_since_recent_payment</th>\n",
       "      <th>time_since_first_deliquency</th>\n",
       "      <th>time_since_recent_deliquency</th>\n",
       "      <th>num_times_delinquent</th>\n",
       "      <th>max_delinquency_level</th>\n",
       "      <th>max_recent_level_of_deliq</th>\n",
       "      <th>num_deliq_6mts</th>\n",
       "      <th>num_deliq_12mts</th>\n",
       "      <th>num_deliq_6_12mts</th>\n",
       "      <th>...</th>\n",
       "      <th>pct_CC_enq_L6m_of_L12m</th>\n",
       "      <th>pct_PL_enq_L6m_of_ever</th>\n",
       "      <th>pct_CC_enq_L6m_of_ever</th>\n",
       "      <th>max_unsec_exposure_inPct</th>\n",
       "      <th>HL_Flag</th>\n",
       "      <th>GL_Flag</th>\n",
       "      <th>last_prod_enq2</th>\n",
       "      <th>first_prod_enq2</th>\n",
       "      <th>Credit_Score</th>\n",
       "      <th>Approved_Flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>549</td>\n",
       "      <td>35</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.333</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PL</td>\n",
       "      <td>PL</td>\n",
       "      <td>696</td>\n",
       "      <td>P2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>47</td>\n",
       "      <td>-99999</td>\n",
       "      <td>-99999</td>\n",
       "      <td>0</td>\n",
       "      <td>-99999</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.860</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ConsumerLoan</td>\n",
       "      <td>ConsumerLoan</td>\n",
       "      <td>685</td>\n",
       "      <td>P2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>302</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5741.667</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>ConsumerLoan</td>\n",
       "      <td>others</td>\n",
       "      <td>693</td>\n",
       "      <td>P2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>-99999</td>\n",
       "      <td>-99999</td>\n",
       "      <td>-99999</td>\n",
       "      <td>0</td>\n",
       "      <td>-99999</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.900</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>others</td>\n",
       "      <td>others</td>\n",
       "      <td>673</td>\n",
       "      <td>P2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>583</td>\n",
       "      <td>-99999</td>\n",
       "      <td>-99999</td>\n",
       "      <td>0</td>\n",
       "      <td>-99999</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-99999.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>AL</td>\n",
       "      <td>AL</td>\n",
       "      <td>753</td>\n",
       "      <td>P1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51331</th>\n",
       "      <td>51332</td>\n",
       "      <td>15</td>\n",
       "      <td>24</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.661</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ConsumerLoan</td>\n",
       "      <td>ConsumerLoan</td>\n",
       "      <td>650</td>\n",
       "      <td>P4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51332</th>\n",
       "      <td>51333</td>\n",
       "      <td>57</td>\n",
       "      <td>-99999</td>\n",
       "      <td>-99999</td>\n",
       "      <td>0</td>\n",
       "      <td>-99999</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.520</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>others</td>\n",
       "      <td>others</td>\n",
       "      <td>702</td>\n",
       "      <td>P1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51333</th>\n",
       "      <td>51334</td>\n",
       "      <td>32</td>\n",
       "      <td>-99999</td>\n",
       "      <td>-99999</td>\n",
       "      <td>0</td>\n",
       "      <td>-99999</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.567</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ConsumerLoan</td>\n",
       "      <td>others</td>\n",
       "      <td>661</td>\n",
       "      <td>P3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51334</th>\n",
       "      <td>51335</td>\n",
       "      <td>58</td>\n",
       "      <td>-99999</td>\n",
       "      <td>-99999</td>\n",
       "      <td>0</td>\n",
       "      <td>-99999</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.202</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ConsumerLoan</td>\n",
       "      <td>others</td>\n",
       "      <td>686</td>\n",
       "      <td>P2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51335</th>\n",
       "      <td>51336</td>\n",
       "      <td>74</td>\n",
       "      <td>-99999</td>\n",
       "      <td>-99999</td>\n",
       "      <td>0</td>\n",
       "      <td>-99999</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-99999.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>others</td>\n",
       "      <td>others</td>\n",
       "      <td>681</td>\n",
       "      <td>P2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>51336 rows √ó 62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       PROSPECTID  time_since_recent_payment  time_since_first_deliquency  \\\n",
       "0               1                        549                           35   \n",
       "1               2                         47                       -99999   \n",
       "2               3                        302                           11   \n",
       "3               4                     -99999                       -99999   \n",
       "4               5                        583                       -99999   \n",
       "...           ...                        ...                          ...   \n",
       "51331       51332                         15                           24   \n",
       "51332       51333                         57                       -99999   \n",
       "51333       51334                         32                       -99999   \n",
       "51334       51335                         58                       -99999   \n",
       "51335       51336                         74                       -99999   \n",
       "\n",
       "       time_since_recent_deliquency  num_times_delinquent  \\\n",
       "0                                15                    11   \n",
       "1                            -99999                     0   \n",
       "2                                 3                     9   \n",
       "3                            -99999                     0   \n",
       "4                            -99999                     0   \n",
       "...                             ...                   ...   \n",
       "51331                            23                     2   \n",
       "51332                        -99999                     0   \n",
       "51333                        -99999                     0   \n",
       "51334                        -99999                     0   \n",
       "51335                        -99999                     0   \n",
       "\n",
       "       max_delinquency_level  max_recent_level_of_deliq  num_deliq_6mts  \\\n",
       "0                         29                         29               0   \n",
       "1                     -99999                          0               0   \n",
       "2                         25                         25               1   \n",
       "3                     -99999                          0               0   \n",
       "4                     -99999                          0               0   \n",
       "...                      ...                        ...             ...   \n",
       "51331                     24                         24               0   \n",
       "51332                 -99999                          0               0   \n",
       "51333                 -99999                          0               0   \n",
       "51334                 -99999                          0               0   \n",
       "51335                 -99999                          0               0   \n",
       "\n",
       "       num_deliq_12mts  num_deliq_6_12mts  ...  pct_CC_enq_L6m_of_L12m  \\\n",
       "0                    0                  0  ...                     0.0   \n",
       "1                    0                  0  ...                     0.0   \n",
       "2                    9                  8  ...                     0.0   \n",
       "3                    0                  0  ...                     0.0   \n",
       "4                    0                  0  ...                     0.0   \n",
       "...                ...                ...  ...                     ...   \n",
       "51331                0                  0  ...                     0.0   \n",
       "51332                0                  0  ...                     0.0   \n",
       "51333                0                  0  ...                     0.0   \n",
       "51334                0                  0  ...                     0.0   \n",
       "51335                0                  0  ...                     0.0   \n",
       "\n",
       "       pct_PL_enq_L6m_of_ever  pct_CC_enq_L6m_of_ever  \\\n",
       "0                         0.0                     0.0   \n",
       "1                         0.0                     0.0   \n",
       "2                         0.0                     0.0   \n",
       "3                         0.0                     0.0   \n",
       "4                         0.0                     0.0   \n",
       "...                       ...                     ...   \n",
       "51331                     0.0                     0.0   \n",
       "51332                     0.0                     0.0   \n",
       "51333                     1.0                     0.0   \n",
       "51334                     0.0                     0.0   \n",
       "51335                     0.0                     0.0   \n",
       "\n",
       "       max_unsec_exposure_inPct  HL_Flag  GL_Flag  last_prod_enq2  \\\n",
       "0                        13.333        1        0              PL   \n",
       "1                         0.860        0        0    ConsumerLoan   \n",
       "2                      5741.667        1        0    ConsumerLoan   \n",
       "3                         9.900        0        0          others   \n",
       "4                    -99999.000        0        0              AL   \n",
       "...                         ...      ...      ...             ...   \n",
       "51331                     1.661        0        0    ConsumerLoan   \n",
       "51332                     0.520        0        0          others   \n",
       "51333                     0.567        0        0    ConsumerLoan   \n",
       "51334                     1.202        0        0    ConsumerLoan   \n",
       "51335                -99999.000        0        0          others   \n",
       "\n",
       "       first_prod_enq2  Credit_Score  Approved_Flag  \n",
       "0                   PL           696             P2  \n",
       "1         ConsumerLoan           685             P2  \n",
       "2               others           693             P2  \n",
       "3               others           673             P2  \n",
       "4                   AL           753             P1  \n",
       "...                ...           ...            ...  \n",
       "51331     ConsumerLoan           650             P4  \n",
       "51332           others           702             P1  \n",
       "51333           others           661             P3  \n",
       "51334           others           686             P2  \n",
       "51335           others           681             P2  \n",
       "\n",
       "[51336 rows x 62 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_to_be_deleted = []\n",
    "for i in external.columns:\n",
    "  if external[external[i] == -99999].shape[0] > 10000:\n",
    "    columns_to_be_deleted.append(i)\n",
    "external    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f002ba89",
   "metadata": {},
   "source": [
    "Vo columns ab hamne hata diye to is se apne dataset train karne k liye acha ho jayega"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82473c3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PROSPECTID</th>\n",
       "      <th>time_since_recent_payment</th>\n",
       "      <th>num_times_delinquent</th>\n",
       "      <th>max_recent_level_of_deliq</th>\n",
       "      <th>num_deliq_6mts</th>\n",
       "      <th>num_deliq_12mts</th>\n",
       "      <th>num_deliq_6_12mts</th>\n",
       "      <th>num_times_30p_dpd</th>\n",
       "      <th>num_times_60p_dpd</th>\n",
       "      <th>num_std</th>\n",
       "      <th>...</th>\n",
       "      <th>pct_PL_enq_L6m_of_L12m</th>\n",
       "      <th>pct_CC_enq_L6m_of_L12m</th>\n",
       "      <th>pct_PL_enq_L6m_of_ever</th>\n",
       "      <th>pct_CC_enq_L6m_of_ever</th>\n",
       "      <th>HL_Flag</th>\n",
       "      <th>GL_Flag</th>\n",
       "      <th>last_prod_enq2</th>\n",
       "      <th>first_prod_enq2</th>\n",
       "      <th>Credit_Score</th>\n",
       "      <th>Approved_Flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>549</td>\n",
       "      <td>11</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PL</td>\n",
       "      <td>PL</td>\n",
       "      <td>696</td>\n",
       "      <td>P2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ConsumerLoan</td>\n",
       "      <td>ConsumerLoan</td>\n",
       "      <td>685</td>\n",
       "      <td>P2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>302</td>\n",
       "      <td>9</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>ConsumerLoan</td>\n",
       "      <td>others</td>\n",
       "      <td>693</td>\n",
       "      <td>P2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>-99999</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>others</td>\n",
       "      <td>others</td>\n",
       "      <td>673</td>\n",
       "      <td>P2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>583</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>AL</td>\n",
       "      <td>AL</td>\n",
       "      <td>753</td>\n",
       "      <td>P1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51331</th>\n",
       "      <td>51332</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ConsumerLoan</td>\n",
       "      <td>ConsumerLoan</td>\n",
       "      <td>650</td>\n",
       "      <td>P4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51332</th>\n",
       "      <td>51333</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>others</td>\n",
       "      <td>others</td>\n",
       "      <td>702</td>\n",
       "      <td>P1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51333</th>\n",
       "      <td>51334</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ConsumerLoan</td>\n",
       "      <td>others</td>\n",
       "      <td>661</td>\n",
       "      <td>P3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51334</th>\n",
       "      <td>51335</td>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ConsumerLoan</td>\n",
       "      <td>others</td>\n",
       "      <td>686</td>\n",
       "      <td>P2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51335</th>\n",
       "      <td>51336</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>others</td>\n",
       "      <td>others</td>\n",
       "      <td>681</td>\n",
       "      <td>P2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>51336 rows √ó 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       PROSPECTID  time_since_recent_payment  num_times_delinquent  \\\n",
       "0               1                        549                    11   \n",
       "1               2                         47                     0   \n",
       "2               3                        302                     9   \n",
       "3               4                     -99999                     0   \n",
       "4               5                        583                     0   \n",
       "...           ...                        ...                   ...   \n",
       "51331       51332                         15                     2   \n",
       "51332       51333                         57                     0   \n",
       "51333       51334                         32                     0   \n",
       "51334       51335                         58                     0   \n",
       "51335       51336                         74                     0   \n",
       "\n",
       "       max_recent_level_of_deliq  num_deliq_6mts  num_deliq_12mts  \\\n",
       "0                             29               0                0   \n",
       "1                              0               0                0   \n",
       "2                             25               1                9   \n",
       "3                              0               0                0   \n",
       "4                              0               0                0   \n",
       "...                          ...             ...              ...   \n",
       "51331                         24               0                0   \n",
       "51332                          0               0                0   \n",
       "51333                          0               0                0   \n",
       "51334                          0               0                0   \n",
       "51335                          0               0                0   \n",
       "\n",
       "       num_deliq_6_12mts  num_times_30p_dpd  num_times_60p_dpd  num_std  ...  \\\n",
       "0                      0                  0                  0       21  ...   \n",
       "1                      0                  0                  0        0  ...   \n",
       "2                      8                  0                  0       10  ...   \n",
       "3                      0                  0                  0        5  ...   \n",
       "4                      0                  0                  0       53  ...   \n",
       "...                  ...                ...                ...      ...  ...   \n",
       "51331                  0                  0                  0        0  ...   \n",
       "51332                  0                  0                  0        6  ...   \n",
       "51333                  0                  0                  0        0  ...   \n",
       "51334                  0                  0                  0        0  ...   \n",
       "51335                  0                  0                  0       18  ...   \n",
       "\n",
       "       pct_PL_enq_L6m_of_L12m  pct_CC_enq_L6m_of_L12m  pct_PL_enq_L6m_of_ever  \\\n",
       "0                         0.0                     0.0                     0.0   \n",
       "1                         0.0                     0.0                     0.0   \n",
       "2                         0.0                     0.0                     0.0   \n",
       "3                         0.0                     0.0                     0.0   \n",
       "4                         0.0                     0.0                     0.0   \n",
       "...                       ...                     ...                     ...   \n",
       "51331                     0.0                     0.0                     0.0   \n",
       "51332                     0.0                     0.0                     0.0   \n",
       "51333                     1.0                     0.0                     1.0   \n",
       "51334                     0.0                     0.0                     0.0   \n",
       "51335                     0.0                     0.0                     0.0   \n",
       "\n",
       "       pct_CC_enq_L6m_of_ever  HL_Flag  GL_Flag  last_prod_enq2  \\\n",
       "0                         0.0        1        0              PL   \n",
       "1                         0.0        0        0    ConsumerLoan   \n",
       "2                         0.0        1        0    ConsumerLoan   \n",
       "3                         0.0        0        0          others   \n",
       "4                         0.0        0        0              AL   \n",
       "...                       ...      ...      ...             ...   \n",
       "51331                     0.0        0        0    ConsumerLoan   \n",
       "51332                     0.0        0        0          others   \n",
       "51333                     0.0        0        0    ConsumerLoan   \n",
       "51334                     0.0        0        0    ConsumerLoan   \n",
       "51335                     0.0        0        0          others   \n",
       "\n",
       "       first_prod_enq2  Credit_Score  Approved_Flag  \n",
       "0                   PL           696             P2  \n",
       "1         ConsumerLoan           685             P2  \n",
       "2               others           693             P2  \n",
       "3               others           673             P2  \n",
       "4                   AL           753             P1  \n",
       "...                ...           ...            ...  \n",
       "51331     ConsumerLoan           650             P4  \n",
       "51332           others           702             P1  \n",
       "51333           others           661             P3  \n",
       "51334           others           686             P2  \n",
       "51335           others           681             P2  \n",
       "\n",
       "[51336 rows x 54 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "external  = external.drop(columns_to_be_deleted,axis=1)\n",
    "external"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9e7563",
   "metadata": {},
   "source": [
    "Ab harr column mien check karke external dataset k vo waale rows hata de rahien hain jinmien null values present hain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16cf2ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in external.columns:\n",
    "  external = external.loc[external[i] != -99999]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1247a2f8",
   "metadata": {},
   "source": [
    "drop karne k baad ki shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac6ef0d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42066"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "external.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96dd8d51",
   "metadata": {},
   "source": [
    "Hamm yahan parr vo waala column dhundh rahien hain jo dono mien common hai taaki dono datasets ko jodke ek single dataset bana sakien "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2eca6017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROSPECTID\n"
     ]
    }
   ],
   "source": [
    "for i in list(external.columns):\n",
    "  if i in list(internal.columns):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b675e4b5",
   "metadata": {},
   "source": [
    "Ab hamien pata chal gaya hai ki prospect id dono dataset mien present hai to ab hamm iske through dono datasets ko merge karke ek single dataset bana detein hain \n",
    "Dono dataset ko join karne se kya hoga \n",
    "- Data analysis acha hoga \n",
    "- more features k saath model train hoga\n",
    "- ek model hamm ek dataset parr hi train karte hain \n",
    "- better results honge  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79835c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.merge(internal,external, how=\"inner\",left_on=\"PROSPECTID\",right_on=\"PROSPECTID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "128f815b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac0a3a5",
   "metadata": {},
   "source": [
    "Ab hamko dekhna hai kounse kounse colums categorical hain to vo kaise dekh saktein hain using dtype command "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d00f9729",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MARITALSTATUS',\n",
       " 'EDUCATION',\n",
       " 'GENDER',\n",
       " 'last_prod_enq2',\n",
       " 'first_prod_enq2',\n",
       " 'Approved_Flag']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_columns = []\n",
    "for i in dataset.columns:\n",
    "  if dataset[i].dtype == 'object':\n",
    "    categorical_columns.append(i)\n",
    "categorical_columns    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27262f9",
   "metadata": {},
   "source": [
    "**üîç Hypothesis Testing between `MARITALSTATUS` and `Approved_Flag`**\n",
    "\n",
    "üéØ Objective\n",
    "- Check karna hai ki **Marital Status** aur **Loan Approval** ke beech koi relationship (association) hai ya nahi.\n",
    "\n",
    "---\n",
    "\n",
    "üß© 1. Hypotheses\n",
    "\n",
    "- **H0 (Null Hypothesis):**  \n",
    "  MARITALSTATUS aur Approved_Flag ke beech koi association nahi hai ‚Üí dono **independent** hain.\n",
    "\n",
    "- **H1 (Alternative Hypothesis):**  \n",
    "  MARITALSTATUS aur Approved_Flag ke beech **association hai** ‚Üí dono **dependent** hain.\n",
    "\n",
    "---\n",
    "\n",
    "‚öôÔ∏è 2. Test Selection\n",
    "\n",
    "- Dono variables **categorical** hain ‚Üí use **Chi-square test of independence (Pearson‚Äôs Chi-square)**.  \n",
    "- Agar kisi bhi expected cell count `< 5` ho:\n",
    "  - To **Fisher‚Äôs Exact Test** use karo (sirf 2√ó2 table ke liye),  \n",
    "  - Ya categories combine kar lo,  \n",
    "  - Ya **Monte Carlo simulation** se p-value nikal lo.\n",
    "\n",
    "---\n",
    "\n",
    "üìè 3. Significance Level (Œ±)\n",
    "\n",
    "| Œ± (alpha) | Strictness | Meaning |\n",
    "|------------|-------------|----------|\n",
    "| 0.01 | High | Bohot strict test |\n",
    "| 0.05 | Moderate | Standard choice |\n",
    "| 0.10 | Low | Thoda loose test |\n",
    "\n",
    "**Decision rule:**\n",
    "- Agar `p ‚â§ Œ±` ‚Üí **Reject H0** ‚Üí association ka evidence hai.  \n",
    "- Agar `p > Œ±` ‚Üí **Fail to reject H0** ‚Üí association ka evidence nahi mila.\n",
    "\n",
    "---\n",
    "\n",
    "üßÆ 4. Test Steps and Formulas\n",
    "\n",
    "- Step 1: Contingency Table\n",
    "\n",
    "Example:\n",
    "\n",
    "| MARITALSTATUS | Approved = Yes | Approved = No | Row Total |\n",
    "|----------------|----------------|----------------|------------|\n",
    "| Married        | O11            | O12            | R1         |\n",
    "| Single         | O21            | O22            | R2         |\n",
    "| **Column Total** | **C1** | **C2** | **N** |\n",
    "\n",
    "---\n",
    "\n",
    "- Step 2: Expected Counts (E_ij)\n",
    "\n",
    "Expected frequency (assuming independence):\n",
    "\n",
    "**E_ij = (Row_i_Total √ó Column_j_Total) / N**\n",
    "\n",
    "---\n",
    "\n",
    "- Step 3: Chi-Square Statistic (X¬≤)\n",
    "\n",
    "**X¬≤ = Œ£ ( (O_ij ‚àí E_ij)¬≤ / E_ij )**\n",
    "\n",
    "‚Üí yahaan O_ij = observed count,  \n",
    "   E_ij = expected count.\n",
    "\n",
    "---\n",
    "\n",
    "- Step 4: Degrees of Freedom (df)\n",
    "\n",
    "**df = (R ‚àí 1) √ó (C ‚àí 1)**  \n",
    "jahan R = number of categories in MARITALSTATUS,  \n",
    "C = number of categories in Approved_Flag.\n",
    "\n",
    "---\n",
    "\n",
    "- Step 5: p-value\n",
    "\n",
    "**p = 1 ‚àí F_chi2(X¬≤, df)**  \n",
    "(yeh Chi-Square distribution se probability nikalta hai)\n",
    "\n",
    "---\n",
    "\n",
    "üìä 5. Effect Size ‚Äî Cram√©r‚Äôs V\n",
    "\n",
    "- Cram√©r‚Äôs V se relationship ki strength measure karte hain:\n",
    "\n",
    "**V = ‚àö( X¬≤ / (N √ó (k ‚àí 1)) )**  \n",
    "jahan k = min(R, C)\n",
    "\n",
    "**Interpretation:**\n",
    "\n",
    "| Cram√©r‚Äôs V | Association Strength |\n",
    "|-------------|----------------------|\n",
    "| ‚âà 0.1 | Small |\n",
    "| ‚âà 0.3 | Medium |\n",
    "| ‚âà 0.5 | Large |\n",
    "\n",
    "---\n",
    " 6. Decision Summary\n",
    "\n",
    "- 1. Contingency table banao  \n",
    "- 2. Expected counts nikal‡•ã  \n",
    "- 3. Chi-square statistic calculate karo  \n",
    "- 4. p-value compare karo Œ± ke saath  \n",
    "- 5. Agar p ‚â§ Œ± ‚Üí association hai  \n",
    "- 6. Cram√©r‚Äôs V se strength check karo\n",
    "\n",
    "---\n",
    "\n",
    "üß† Example Interpretation\n",
    "\n",
    "- Agar **p = 0.03** aur **Œ± = 0.05**, to **Reject H0** ‚Üí Marital Status aur Loan Approval ke beech relation hai.  \n",
    "- Agar **p = 0.20**, to **Fail to reject H0** ‚Üí association ka evidence nahi hai.\n",
    "\n",
    "---\n",
    "\n",
    "üìò **In short:**  \n",
    "Chi-square test se hum check karte hain ki kya categorical variables ek-dusre se independent hain ya nahi.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb37e19",
   "metadata": {},
   "source": [
    "üéØ Interview Question (Data Science)\n",
    "\n",
    "**Q:**  \n",
    "Chi-Square test (ya kisi bhi hypothesis test) mein hum \"Accept H‚ÇÄ\" kyu nahi likhte,  \n",
    "balki **\"Fail to reject H‚ÇÄ\"** likhte hain?\n",
    "\n",
    "---\n",
    "\n",
    "üß† **Answer (with Court of Law Analogy)**\n",
    "\n",
    "Socho ki hypothesis testing ek **court case** jaisa hai üë©‚Äç‚öñÔ∏è‚öñÔ∏è  \n",
    "\n",
    "- **H‚ÇÄ (Null Hypothesis)** ‚Üí \"Insaan **guilty nahi hai**\"  \n",
    "- **H‚ÇÅ (Alternative Hypothesis)** ‚Üí \"Insaan **guilty hai**\"\n",
    "\n",
    "Ab court mein judge kya karta hai?  \n",
    "- Jab tak **pura evidence (proof)** nahi milta,  \n",
    "  tab tak judge **guilty declare nahi karta**.  \n",
    "- Lekin judge yeh bhi nahi kehta ki \"wo 100% innocent hai\",  \n",
    "  bas itna kehta hai ‚Äî **\"Evidence ke basis par, guilty prove nahi hua.\"**\n",
    "\n",
    "Exactly wahi logic hypothesis testing mein lagta hai üëá\n",
    "\n",
    "---\n",
    "\n",
    "üí° Statistical Analogy\n",
    "\n",
    "| Court Example | Hypothesis Test Equivalent |\n",
    "|----------------|-----------------------------|\n",
    "| Defendant is **innocent** until proven guilty | Null Hypothesis (H‚ÇÄ) is **true** until proven otherwise |\n",
    "| Court **rejects innocence** if strong evidence (proof) milta hai | We **reject H‚ÇÄ** if p ‚â§ Œ± (strong evidence against H‚ÇÄ) |\n",
    "| If evidence **not strong**, judge says \"Not Proven\" (not \"Innocent\") | We say **\"Fail to reject H‚ÇÄ\"**, not **\"Accept H‚ÇÄ\"** |\n",
    "\n",
    "---\n",
    "\n",
    "üß© **Reasoning Summary**\n",
    "\n",
    "- Hum **\"Accept H‚ÇÄ\"** nahi kehte kyunki  \n",
    "  sample data se hum **100% sure** nahi ho sakte ki H‚ÇÄ sach hai.  \n",
    "- Hum sirf ye keh sakte hain ki  \n",
    "  *‚Äúavailable data ke basis par H‚ÇÄ ko reject karne ke liye evidence strong nahi hai.‚Äù*\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ **Example**\n",
    "\n",
    "Suppose hum test kar rahe hain:\n",
    "\n",
    "> H‚ÇÄ: Loan approval aur Marital status independent hain  \n",
    "> H‚ÇÅ: Loan approval aur Marital status dependent hain\n",
    "\n",
    "Agar **p-value = 0.20** hai (which is > 0.05),  \n",
    "to hum kehte hain ‚Äî  \n",
    "> ‚ÄúWe fail to reject H‚ÇÄ‚Äù  \n",
    "> (i.e., data se association ka strong proof nahi mila)\n",
    "\n",
    "Not: ‚ÄúWe accept H‚ÇÄ‚Äù ‚Äî kyunki proof ki kami **acceptance ka proof nahi hoti**.\n",
    "\n",
    "---\n",
    "\n",
    "**In short:**  \n",
    "üëâ Hypothesis testing mein hum **‚Äúfail to reject H‚ÇÄ‚Äù** likhte hain,  \n",
    "kyunki hum bas yeh bol sakte hain ki **evidence insufficient hai**,  \n",
    "not that **H‚ÇÄ definitely true hai.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bec7e4fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MARITALSTATUS ---- 3.578180861038862e-233\n",
      "EDUCATION ---- 2.6942265249737532e-30\n",
      "GENDER ---- 1.907936100186563e-05\n",
      "last_prod_enq2 ---- 0.0\n",
      "first_prod_enq2 ---- 7.84997610555419e-287\n"
     ]
    }
   ],
   "source": [
    "# Chi-Square Test \n",
    "for i in ['MARITALSTATUS',\n",
    " 'EDUCATION',\n",
    " 'GENDER',\n",
    " 'last_prod_enq2',\n",
    " 'first_prod_enq2']:\n",
    "  chi2,pval,_,_ =chi2_contingency(pd.crosstab(dataset[i],dataset['Approved_Flag']))\n",
    "  print(i,'----',pval)\n",
    "\n",
    "# Ab Sabhi ka pval value jo hai vo less than 0.05 to fir hamm sabhi ko accept kar lenge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0d8e15",
   "metadata": {},
   "source": [
    " üîç Multicollinearity vs Correlation\n",
    "\n",
    "---\n",
    "\n",
    " üß† **1. Correlation kya hota hai?**\n",
    "\n",
    "**Definition:**  \n",
    "Correlation ek **statistical measure** hai jo do variables ke beech **linear relationship** batata hai.  \n",
    "Agar ek variable badhta hai to doosra bhi badhta ya ghatta hai ‚Äî ye correlation direction aur strength batata hai.\n",
    "\n",
    "**Formula:**\n",
    "r = Cov(X, Y) / (œÉ‚Çì √ó œÉ·µß)\n",
    "\n",
    "**Range:** -1 se +1 tak  \n",
    "- +1 ‚Üí perfect positive linear relationship  \n",
    "- -1 ‚Üí perfect negative linear relationship  \n",
    "- 0 ‚Üí no linear relationship\n",
    "\n",
    "**Important Point (Interview Tip ‚ö°):**  \n",
    "Correlation **sirf linear relationships** detect karta hai.  \n",
    "Agar variables ka relationship **non-linear (jaise convex ya concave function)** ho,  \n",
    "to correlation **misleading ya near-zero** bhi aa sakta hai ‚Äî  \n",
    "even when a strong relationship actually exists!\n",
    "\n",
    "üëâ Example:  \n",
    "Y = X¬≤ (convex curve)  \n",
    "Correlation between X and Y ‚âà 0,  \n",
    "but clearly Y depends on X ‚Äî correlation ne linear assumption ke wajah se galat impression diya.\n",
    "\n",
    "---\n",
    "\n",
    " ‚öôÔ∏è **2. Multicollinearity kya hota hai?**\n",
    "\n",
    "**Definition:**  \n",
    "Multicollinearity tab hota hai jab ek **independent variable** model ke doosre **independent variables** se  \n",
    "**predict kiya ja sakta hai** (i.e., one predictor can be linearly predicted from others).  \n",
    "\n",
    "üëâ Iska matlab:  \n",
    "Multicollinearity = **Predictability of each feature by other features**\n",
    "\n",
    "**Example:**  \n",
    "`Age`, `Experience`, aur `Years_in_Workforce`  \n",
    "ye tino variables ek-dusre ke linear combination ho sakte hain ‚Üí multicollinearity.\n",
    "\n",
    "---\n",
    "\n",
    " üö® **3. Problems caused by Multicollinearity**\n",
    "\n",
    "1. Coefficients unstable ho jaate hain (Œ≤ values flip kar sakte hain).  \n",
    "2. Standard errors high ‚Üí p-values unreliable.  \n",
    "3. Model interpret karna mushkil ho jaata hai.  \n",
    "4. Predictions me slight instability aati hai (especially small data pe).\n",
    "\n",
    "---\n",
    "\n",
    " üìä **4. How to Detect Multicollinearity**\n",
    "\n",
    "1. **Correlation Matrix**  \n",
    "   - Pairwise correlation check karo independent variables ke beech.  \n",
    "   - Agar r > 0.8 (ya < -0.8) ‚Üí potential issue.  \n",
    "   - Lekin: correlation ‚â† multicollinearity (sirf hint milta hai).\n",
    "\n",
    "2. **VIF (Variance Inflation Factor)**  \n",
    "   - Formula: VIF = 1 / (1 - R¬≤)\n",
    "   - Agar VIF > 5 ‚Üí moderate multicollinearity  \n",
    "   - Agar VIF > 10 ‚Üí severe multicollinearity\n",
    "\n",
    "---\n",
    "\n",
    " üß© **5. Difference between Correlation and Multicollinearity**\n",
    "\n",
    "| Feature | **Correlation** | **Multicollinearity** |\n",
    "|----------|------------------|------------------------|\n",
    "| **Meaning** | Relationship between **two variables** | Predictability of **one independent variable** by other independent variables |\n",
    "| **Scope** | Pairwise (2 variables only) | Multiple predictors together |\n",
    "| **Type of Relationship** | Captures only **linear** relationship | Can exist even if predictors are **linearly dependent** on each other |\n",
    "| **Effect on Model** | Sirf understanding ke liye | Affects regression model interpretation |\n",
    "| **Detection** | Correlation coefficient (r) | VIF, Tolerance, Eigenvalues |\n",
    "| **Misleading Case** | In **non-linear (convex/concave)** relationships, correlation ‚âà 0 despite strong link | Not applicable |\n",
    "| **Interpretation** | Measures direction & strength of relation | Measures redundancy among predictors |\n",
    "\n",
    "---\n",
    "\n",
    " üí° **6. In Short**\n",
    "\n",
    "- **Correlation** ‚Üí measures **linear relationship** between two variables.  \n",
    "- **Multicollinearity** ‚Üí measures **how well one predictor can be predicted** using other predictors.  \n",
    "- Correlation can be **zero even if relationship exists (non-linear case)**.  \n",
    "- Multicollinearity is **problematic for regression models** ‚Äî leads to unstable coefficients.\n",
    "\n",
    "---\n",
    "\n",
    " ‚úÖ **7. Handling Multicollinearity**\n",
    "\n",
    "1. Remove or merge correlated features  \n",
    "2. Use **Ridge / Lasso Regression** (Regularization)  \n",
    "3. Use **PCA (Principal Component Analysis)**  \n",
    "4. Drop variables with high VIF values\n",
    "\n",
    "---\n",
    "\n",
    " üó£Ô∏è **Interview Summary Answer**\n",
    "\n",
    "> \"Sir, correlation bas linear relationship dikhata hai do variables ke beech,  \n",
    "> lekin multicollinearity regression ke predictors ke beech dependency dikhata hai.  \n",
    "> Multicollinearity basically predictability hoti hai ek feature ki doosre features se.  \n",
    "> Aur correlation kabhi-kabhi misleading ho sakta hai ‚Äî  \n",
    "> especially non-linear ya convex functions mein, jahan correlation near-zero aata hai  \n",
    "> but relationship actually strong hota hai.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53e39952",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Numerical columns other than PROSPECTID for VIF\n",
    "numerical_columns = []\n",
    "for i in dataset.columns:\n",
    "  if dataset[i].dtype != 'object' and i not in [\"PROSPECTID\"]:\n",
    "    numerical_columns.append(i)\n",
    "len(numerical_columns)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e730bc",
   "metadata": {},
   "source": [
    "VIF Sequentially check karenge each numerical column k liye agar \n",
    "- 1. VIF <=6 aayega to us column drop nahi karenge \n",
    "- 2. VIF >6 aayega to us column ko drop kar denge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8d6ecf",
   "metadata": {},
   "source": [
    " üßÆ VIF (Variance Inflation Factor): Parallel vs Sequential Approach\n",
    "\n",
    " üîπ Parallel VIF Approach\n",
    "- Saare columns ka **VIF ek saath calculate** hota hai.\n",
    "- Agar kisi feature ka VIF threshold (e.g., 5 ya 10) se zyada hota hai ‚Üí **wo feature hata dete hain**.\n",
    "- Example: Agar `A`, `B`, `C` tino ek dusre se correlated hain,  \n",
    "  to ye approach **tino ko hata sakti hai**, kyunki sabka VIF high hoga.\n",
    "- ‚ùå **Problem:** Isse useful information loss ho sakti hai.\n",
    "\n",
    " ‚ö° Key Points\n",
    "- One-shot calculation  \n",
    "- Fast execution  \n",
    "- May remove all correlated features  \n",
    "- Risk of high information loss  \n",
    "\n",
    "---\n",
    "\n",
    " üîπ Sequential (Iterative) VIF Approach\n",
    "- Step-by-step kaam karta hai.\n",
    "- Pehle sab features ka VIF nikalta hai ‚Üí **highest VIF wala column drop karta hai**.\n",
    "- Phir remaining features ke liye VIF dobara calculate karta hai.\n",
    "- Ye process tab tak repeat hoti hai jab tak sabka VIF threshold ke neeche nahi aata.\n",
    "- ‚úÖ **Benefit:** Ek representative feature bach jaata hai correlated group me se.\n",
    "\n",
    " ‚ö° Key Points\n",
    "- Iterative calculation  \n",
    "- Slower than parallel  \n",
    "- Keeps one representative feature from correlated group  \n",
    "- Less data loss  \n",
    "\n",
    "---\n",
    "\n",
    " ‚öñÔ∏è Comparison Table\n",
    "\n",
    "| Feature | Parallel | Sequential |\n",
    "|----------|-----------|-------------|\n",
    "| VIF Calculation | One-shot | Iterative |\n",
    "| Speed | Fast | Slow |\n",
    "| Feature Retention | May drop all correlated features | Keeps one representative |\n",
    "| Risk | High data loss | Low data loss |\n",
    "| Recommended for | Quick screening | Final feature selection |\n",
    "\n",
    "---\n",
    "\n",
    " üß© Conclusion\n",
    "üëâ **Sequential VIF** thoda slow hai, par **intelligently correlated features ko handle karta hai**,  \n",
    "isliye **real-world regression modeling** ke liye **better choice** hai.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2de1aee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ankitmaan/gpu_env/lib/python3.12/site-packages/statsmodels/stats/outliers_influence.py:197: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  vif = 1. / (1. - r_squared_i)\n",
      "/tmp/ipykernel_20643/3371107494.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  VIFdataset.drop([numerical_columns[i]],axis=1,inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 --- inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ankitmaan/gpu_env/lib/python3.12/site-packages/statsmodels/stats/outliers_influence.py:197: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  vif = 1. / (1. - r_squared_i)\n",
      "/tmp/ipykernel_20643/3371107494.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  VIFdataset.drop([numerical_columns[i]],axis=1,inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 --- inf\n",
      "0 --- 11.320180023967996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20643/3371107494.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  VIFdataset.drop([numerical_columns[i]],axis=1,inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 --- 8.363698035000336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20643/3371107494.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  VIFdataset.drop([numerical_columns[i]],axis=1,inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 --- 6.520647877790928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20643/3371107494.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  VIFdataset.drop([numerical_columns[i]],axis=1,inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 --- 5.149501618212625\n",
      "1 --- 2.611111040579735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ankitmaan/gpu_env/lib/python3.12/site-packages/statsmodels/stats/outliers_influence.py:197: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  vif = 1. / (1. - r_squared_i)\n",
      "/tmp/ipykernel_20643/3371107494.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  VIFdataset.drop([numerical_columns[i]],axis=1,inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 --- inf\n",
      "2 --- 1788.7926256209232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20643/3371107494.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  VIFdataset.drop([numerical_columns[i]],axis=1,inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 --- 8.601028256477228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20643/3371107494.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  VIFdataset.drop([numerical_columns[i]],axis=1,inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 --- 3.8328007921530785\n",
      "3 --- 6.099653381646739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20643/3371107494.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  VIFdataset.drop([numerical_columns[i]],axis=1,inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 --- 5.5813520096427585\n",
      "4 --- 1.985584353098778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ankitmaan/gpu_env/lib/python3.12/site-packages/statsmodels/stats/outliers_influence.py:197: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  vif = 1. / (1. - r_squared_i)\n",
      "/tmp/ipykernel_20643/3371107494.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  VIFdataset.drop([numerical_columns[i]],axis=1,inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 --- inf\n",
      "5 --- 4.809538302819343\n",
      "6 --- 23.270628983464636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20643/3371107494.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  VIFdataset.drop([numerical_columns[i]],axis=1,inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 --- 30.595522588100053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20643/3371107494.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  VIFdataset.drop([numerical_columns[i]],axis=1,inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 --- 4.384346405965587\n",
      "7 --- 3.0646584155234247\n",
      "8 --- 2.898639771299251\n",
      "9 --- 4.377876915347319\n",
      "10 --- 2.2078535836958433\n",
      "11 --- 4.916914200506864\n",
      "12 --- 5.214702030064725\n",
      "13 --- 3.3861625024231476\n",
      "14 --- 7.840583309478997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20643/3371107494.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  VIFdataset.drop([numerical_columns[i]],axis=1,inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 --- 5.255034641721438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ankitmaan/gpu_env/lib/python3.12/site-packages/statsmodels/stats/outliers_influence.py:197: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  vif = 1. / (1. - r_squared_i)\n",
      "/tmp/ipykernel_20643/3371107494.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  VIFdataset.drop([numerical_columns[i]],axis=1,inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 --- inf\n",
      "15 --- 7.380634506427232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20643/3371107494.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  VIFdataset.drop([numerical_columns[i]],axis=1,inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 --- 1.421005001517573\n",
      "16 --- 8.083255010190323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20643/3371107494.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  VIFdataset.drop([numerical_columns[i]],axis=1,inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 --- 1.6241227524040114\n",
      "17 --- 7.257811920140003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20643/3371107494.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  VIFdataset.drop([numerical_columns[i]],axis=1,inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 --- 15.59624383268298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20643/3371107494.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  VIFdataset.drop([numerical_columns[i]],axis=1,inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 --- 1.825857047132431\n",
      "18 --- 1.5080839450032664\n",
      "19 --- 2.172088834824577\n",
      "20 --- 2.623397553527229\n",
      "21 --- 2.2959970812106167\n",
      "22 --- 7.360578319196439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20643/3371107494.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  VIFdataset.drop([numerical_columns[i]],axis=1,inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22 --- 2.1602387773102554\n",
      "23 --- 2.8686288267891467\n",
      "24 --- 6.458218003637272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20643/3371107494.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  VIFdataset.drop([numerical_columns[i]],axis=1,inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 --- 2.8474118865638265\n",
      "25 --- 4.753198156284083\n",
      "26 --- 16.22735475594825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20643/3371107494.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  VIFdataset.drop([numerical_columns[i]],axis=1,inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26 --- 6.424377256363877\n",
      "26 --- 8.887080381808687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20643/3371107494.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  VIFdataset.drop([numerical_columns[i]],axis=1,inplace=True)\n",
      "/tmp/ipykernel_20643/3371107494.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  VIFdataset.drop([numerical_columns[i]],axis=1,inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26 --- 2.3804746142952653\n",
      "27 --- 8.609513476514548\n",
      "27 --- 13.06755093547673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20643/3371107494.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  VIFdataset.drop([numerical_columns[i]],axis=1,inplace=True)\n",
      "/tmp/ipykernel_20643/3371107494.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  VIFdataset.drop([numerical_columns[i]],axis=1,inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27 --- 3.5000400566546555\n",
      "28 --- 1.9087955874813773\n",
      "29 --- 17.006562234161628\n",
      "29 --- 10.730485153719197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20643/3371107494.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  VIFdataset.drop([numerical_columns[i]],axis=1,inplace=True)\n",
      "/tmp/ipykernel_20643/3371107494.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  VIFdataset.drop([numerical_columns[i]],axis=1,inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29 --- 2.3538497522950275\n",
      "30 --- 22.104855915136433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20643/3371107494.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  VIFdataset.drop([numerical_columns[i]],axis=1,inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 --- 2.7971639638512906\n",
      "31 --- 3.424171203217696\n",
      "32 --- 10.175021454450935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20643/3371107494.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  VIFdataset.drop([numerical_columns[i]],axis=1,inplace=True)\n",
      "/tmp/ipykernel_20643/3371107494.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  VIFdataset.drop([numerical_columns[i]],axis=1,inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 --- 6.408710354561301\n",
      "32 --- 1.0011511962625619\n",
      "33 --- 3.069197305397274\n",
      "34 --- 2.8091261600643724\n",
      "35 --- 20.249538381980678\n",
      "35 --- 15.864576541593774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20643/3371107494.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  VIFdataset.drop([numerical_columns[i]],axis=1,inplace=True)\n",
      "/tmp/ipykernel_20643/3371107494.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  VIFdataset.drop([numerical_columns[i]],axis=1,inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35 --- 1.8331649740532168\n",
      "36 --- 1.5680839909542037\n",
      "37 --- 1.9307572353811677\n",
      "38 --- 4.331265056645247\n",
      "39 --- 9.390334396150173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20643/3371107494.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  VIFdataset.drop([numerical_columns[i]],axis=1,inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# VIF CHECK \n",
    "VIFdataset = dataset[numerical_columns]\n",
    "totalColumns = len(numerical_columns)\n",
    "preservedColumns = []\n",
    "columnIndex = 0\n",
    "\n",
    "\n",
    "for i in range(0,totalColumns):\n",
    "  VIF = variance_inflation_factor(VIFdataset,columnIndex)\n",
    "  print(f\"{columnIndex} --- {VIF}\")\n",
    "\n",
    "  if VIF<=6:\n",
    "    columnIndex +=1\n",
    "    preservedColumns.append(numerical_columns[i])\n",
    "  else:\n",
    "    VIFdataset.drop([numerical_columns[i]],axis=1,inplace=True)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a8b224",
   "metadata": {},
   "source": [
    "üßÆ ANOVA Test (Analysis of Variance)\n",
    "\n",
    "üéØ Objective\n",
    "ANOVA (Analysis of Variance) ek **statistical test** hai jo check karta hai ki \n",
    "**kya multiple groups ke means (averages) ek jaise hain ya unmein significant difference hai.**\n",
    "\n",
    "In other words:\n",
    "> ANOVA test help karta hai ye determine karne mein ki **ek numerical feature** aur **categorical target** ke beech koi **significant relationship (association)** hai ya nahi.\n",
    "\n",
    "---\n",
    "\n",
    "üß† Basic Idea\n",
    "Suppose tumhare paas 4 groups hain based on `Approved_Flag`:\n",
    "- P1 ‚Üí Approved customers  \n",
    "- P2 ‚Üí Rejected customers  \n",
    "- P3 ‚Üí Pending customers  \n",
    "- P4 ‚Üí Others  \n",
    "\n",
    "Aur ek numerical feature hai (for example, `ApplicantIncome`).  \n",
    "ANOVA test karega:\n",
    "> Kya `ApplicantIncome` ka **mean value** in 4 groups ke liye **same hai**  \n",
    "> ya koi **statistically significant difference** hai?\n",
    "\n",
    "---\n",
    "\n",
    "üß© Hypotheses\n",
    "\n",
    "| Type | Statement |\n",
    "|------|------------|\n",
    "| **H‚ÇÄ (Null Hypothesis)** | Group means are equal ‚Üí No significant difference ‚Üí Feature not related to target. |\n",
    "| **H‚ÇÅ (Alternative Hypothesis)** | At least one group mean is different ‚Üí Feature related to target. |\n",
    "\n",
    "---\n",
    "\n",
    "‚öôÔ∏è Test Statistic (F-ratio)\n",
    "ANOVA F-test compare karta hai:\n",
    "\\[\n",
    "F = \\frac{\\text{Between-group variance}}{\\text{Within-group variance}}\n",
    "\\]\n",
    "\n",
    "- **Between-group variance:** Differences in means between groups  \n",
    "- **Within-group variance:** Variability within each group  \n",
    "\n",
    "Agar **F-statistic large hai** aur **p-value ‚â§ 0.05**,  \n",
    "to iska matlab:\n",
    "> Group means mein significant difference hai ‚Üí Feature and Target related hain.\n",
    "\n",
    "---\n",
    "\n",
    "üíª Code Explanation (Credit Risk Modelling Context)\n",
    "\n",
    "```python\n",
    "# Initialize a list to keep only statistically significant features\n",
    "safeColumns = []\n",
    "\n",
    "# Loop over all columns that survived after VIF (multicollinearity removal)\n",
    "for i in preservedColumns:\n",
    "    a = list(dataset[i])                 # Feature column values\n",
    "    b = list(dataset['Approved_Flag'])   # Target column (categorical groups)\n",
    "\n",
    "    # Split feature values based on each group in Approved_Flag\n",
    "    groupP1 = [value for value, group in zip(a, b) if group == 'P1']\n",
    "    groupP2 = [value for value, group in zip(a, b) if group == 'P2']\n",
    "    groupP3 = [value for value, group in zip(a, b) if group == 'P3']\n",
    "    groupP4 = [value for value, group in zip(a, b) if group == 'P4']\n",
    "\n",
    "    # Perform one-way ANOVA test across 4 groups\n",
    "    f_statistics, pvalue = f_oneway(groupP1, groupP2, groupP3, groupP4)\n",
    "\n",
    "    # If feature is statistically significant, keep it\n",
    "    if pvalue <= 0.05:\n",
    "        safeColumns.append(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19c295f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annova Test\n",
    "safeColumns = []\n",
    "for i in preservedColumns:\n",
    "  a = list(dataset[i])\n",
    "  b = list(dataset['Approved_Flag'])\n",
    "\n",
    "  groupP1 = [value for value, group in zip(a,b) if group == 'P1']\n",
    "  groupP2 = [value for value, group in zip(a,b) if group == 'P2']\n",
    "  groupP3 = [value for value, group in zip(a,b) if group == 'P3']\n",
    "  groupP4 = [value for value, group in zip(a,b) if group == 'P4']\n",
    "\n",
    "  f_statistics, pvalue = f_oneway(groupP1,groupP2,groupP3,groupP4)\n",
    "\n",
    "  if pvalue <=0.05:\n",
    "    safeColumns.append(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fac4279e",
   "metadata": {},
   "outputs": [],
   "source": [
    "finalColumns = safeColumns + categorical_columns\n",
    "dataset = dataset.loc[:,finalColumns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ac9fb3",
   "metadata": {},
   "source": [
    "Categorical Variables ko encode karenge variable k form mien\n",
    "- Education ordinal variable hai to ismien ranking hai to label encoding karenge\n",
    "- Bache hue categorical variables ko hamm one hot encoding karenge using **pd.getdummies** \n",
    "  - **pd.getdummies ka kaam hai jitni bhi categorical column mien unique values hoti hain un sabka alag se column bana deta hai with values true yaa false or agar dtype int provide karte ho to 0 or 1 daal deta hai**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "594e64bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Married' 'Single']\n",
      "['12TH' 'GRADUATE' 'SSC' 'POST-GRADUATE' 'UNDER GRADUATE' 'OTHERS'\n",
      " 'PROFESSIONAL']\n",
      "['M' 'F']\n",
      "['PL' 'ConsumerLoan' 'AL' 'CC' 'others' 'HL']\n",
      "['PL' 'ConsumerLoan' 'others' 'AL' 'HL' 'CC']\n"
     ]
    }
   ],
   "source": [
    "#Encoding for categorical Features \n",
    "print(dataset['MARITALSTATUS'].unique())\n",
    "print(dataset['EDUCATION'].unique())\n",
    "print(dataset['GENDER'].unique())\n",
    "print(dataset['last_prod_enq2'].unique())\n",
    "print(dataset['first_prod_enq2'].unique())\n",
    "\n",
    "#First Lets deal with Education column because it is an ordinal column so4\n",
    "dataset.loc[dataset['EDUCATION']=='SSC',['EDUCATION']]=1\n",
    "dataset.loc[dataset['EDUCATION']=='12TH',['EDUCATION']]=2\n",
    "dataset.loc[dataset['EDUCATION']=='OTHERS',['EDUCATION']]=1\n",
    "dataset.loc[dataset['EDUCATION']=='GRADUATE',['EDUCATION']]=3\n",
    "dataset.loc[dataset['EDUCATION']=='UNDER GRADUATE',['EDUCATION']]=3\n",
    "dataset.loc[dataset['EDUCATION']=='POST-GRADUATE',['EDUCATION']]=4\n",
    "dataset.loc[dataset['EDUCATION']=='PROFESSIONAL',['EDUCATION']]=3\n",
    "\n",
    "dataset['EDUCATION'] = dataset.EDUCATION.astype(int)\n",
    "\n",
    "#Other than Education column, for all columns we can do one-hot encoding \n",
    "datasetEncoded = pd.get_dummies(dataset, columns=['MARITALSTATUS','GENDER','last_prod_enq2','first_prod_enq2'],dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2671d894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pct_tl_open_L6M', 'pct_tl_closed_L6M', 'Tot_TL_closed_L12M', 'pct_tl_closed_L12M', 'Tot_Missed_Pmnt', 'CC_TL', 'Home_TL', 'PL_TL', 'Secured_TL', 'Unsecured_TL', 'Other_TL', 'Age_Oldest_TL', 'Age_Newest_TL', 'time_since_recent_payment', 'max_recent_level_of_deliq', 'num_deliq_6_12mts', 'num_times_60p_dpd', 'num_std_12mts', 'num_sub', 'num_sub_6mts', 'num_sub_12mts', 'num_dbt', 'num_dbt_12mts', 'num_lss', 'recent_level_of_deliq', 'CC_enq_L12m', 'PL_enq_L12m', 'time_since_recent_enq', 'enq_L3m', 'NETMONTHLYINCOME', 'Time_With_Curr_Empr', 'CC_Flag', 'PL_Flag', 'pct_PL_enq_L6m_of_ever', 'pct_CC_enq_L6m_of_ever', 'HL_Flag', 'GL_Flag', 'EDUCATION', 'Approved_Flag', 'MARITALSTATUS_Married', 'MARITALSTATUS_Single', 'GENDER_F', 'GENDER_M', 'last_prod_enq2_AL', 'last_prod_enq2_CC', 'last_prod_enq2_ConsumerLoan', 'last_prod_enq2_HL', 'last_prod_enq2_PL', 'last_prod_enq2_others', 'first_prod_enq2_AL', 'first_prod_enq2_CC', 'first_prod_enq2_ConsumerLoan', 'first_prod_enq2_HL', 'first_prod_enq2_PL', 'first_prod_enq2_others']\n"
     ]
    }
   ],
   "source": [
    "datasetEncoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480188bc",
   "metadata": {},
   "source": [
    "To yahan pe jaake hamara feature engineering jaake complete hua \n",
    "\n",
    "Note :\n",
    "- Pahle model ka performance bina feature scaling k dekho \n",
    "- Fir us model ka performance feature scaling k saath dekho \n",
    "- Agar model performance before and after same hai to no need of feature scaling otherwise there is need"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b44e90",
   "metadata": {},
   "source": [
    "**Model Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fbb26523",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "y = datasetEncoded['Approved_Flag']\n",
    "x = datasetEncoded.drop(['Approved_Flag'],axis=1)\n",
    "# label encoding of y \n",
    "labelencoder = LabelEncoder()\n",
    "yencoded = labelencoder.fit_transform(y)\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,yencoded,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b8be2d",
   "metadata": {},
   "source": [
    "Random Forest ML Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d6b04394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest Results\n",
      "accuracy score : 0.7636990372043266\n",
      "Classp1\n",
      "Precision: 0.8370457209847597\n",
      "recall: 0.7041420118343196\n",
      "F1_score: 0.7648634172469202\n",
      "\n",
      "Classp2\n",
      "Precision: 0.7957519116397621\n",
      "recall: 0.9282457879088206\n",
      "F1_score: 0.856907593778591\n",
      "\n",
      "Classp3\n",
      "Precision: 0.4423380726698262\n",
      "recall: 0.21132075471698114\n",
      "F1_score: 0.28600612870275793\n",
      "\n",
      "Classp4\n",
      "Precision: 0.7178502879078695\n",
      "recall: 0.7269193391642371\n",
      "F1_score: 0.7223563495895703\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def RandomForest(x_train,y_train,x_test,y_test):\n",
    "  model = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "  model.fit(x_train,y_train)\n",
    "  y_pred = model.predict(x_test)\n",
    "  print(\"RandomForest Results\")\n",
    "  print(\"accuracy score :\",accuracy_score(y_test,y_pred))\n",
    "  precision, recall, f1_score, _ = precision_recall_fscore_support(y_test,y_pred)\n",
    "  for i, v in enumerate(['p1','p2','p3','p4']):\n",
    "    print(f\"Class{v}\")\n",
    "    print(f\"Precision: {precision[i]}\")\n",
    "    print(f\"recall: {recall[i]}\")\n",
    "    print(f\"F1_score: {f1_score[i]}\")\n",
    "    print()\n",
    "RandomForest(x_train,y_train,x_test,y_test)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695d4686",
   "metadata": {},
   "source": [
    "XGBOOST ML Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fa499253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBOOST Results\n",
      "accuracy score : 0.7784381314632117\n",
      "Classp1\n",
      "Precision: 0.8268215417106652\n",
      "recall: 0.772189349112426\n",
      "F1_score: 0.7985721570627231\n",
      "\n",
      "Classp2\n",
      "Precision: 0.8234455727774809\n",
      "recall: 0.9161546085232903\n",
      "F1_score: 0.8673297053856258\n",
      "\n",
      "Classp3\n",
      "Precision: 0.4673913043478261\n",
      "recall: 0.2920754716981132\n",
      "F1_score: 0.35949837436135623\n",
      "\n",
      "Classp4\n",
      "Precision: 0.7385365853658536\n",
      "recall: 0.7356656948493683\n",
      "F1_score: 0.7370983446932814\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "def XGBOOST(x_train,y_train,x_test,y_test):\n",
    "  model = xgb.XGBClassifier(objective='multi:softmax',num_class=4,colsample_bytree =0.9 , learning_rate =0.1 ,\tmax_depth=10,\talpha =10 ,\tn_estimators =100 )\n",
    "  model.fit(x_train,y_train)\n",
    "  y_pred = model.predict(x_test)\n",
    "  print(\"XGBOOST Results\")\n",
    "  print(\"accuracy score :\",accuracy_score(y_test,y_pred))\n",
    "  precision, recall, f1_score, _ = precision_recall_fscore_support(y_test,y_pred)\n",
    "  for i, v in enumerate(['p1','p2','p3','p4']):\n",
    "    print(f\"Class{v}\")\n",
    "    print(f\"Precision: {precision[i]}\")\n",
    "    print(f\"recall: {recall[i]}\")\n",
    "    print(f\"F1_score: {f1_score[i]}\")\n",
    "    print()\n",
    "XGBOOST(x_train,y_train,x_test,y_test)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bd2d53",
   "metadata": {},
   "source": [
    "Decision Tree ML Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3cb3ae54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Results\n",
      "accuracy score: 0.7080708427433734\n",
      "Classp1\n",
      "Precision: 0.7174757281553398\n",
      "recall: 0.7287968441814595\n",
      "F1_score: 0.723091976516634\n",
      "\n",
      "Classp2\n",
      "Precision: 0.8094122241749658\n",
      "recall: 0.821605550049554\n",
      "F1_score: 0.8154633090694472\n",
      "\n",
      "Classp3\n",
      "Precision: 0.3383280757097792\n",
      "recall: 0.3237735849056604\n",
      "F1_score: 0.33089086000771306\n",
      "\n",
      "Classp4\n",
      "Precision: 0.647887323943662\n",
      "recall: 0.6258503401360545\n",
      "F1_score: 0.6366782006920415\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "def DecisionTree(x_train,y_train,x_test,y_test):\n",
    "  model = DecisionTreeClassifier(max_depth=20,min_samples_split=10)\n",
    "  model.fit(x_train,y_train)\n",
    "  y_pred = model.predict(x_test)\n",
    "  print(\"Decision Tree Results\")\n",
    "  print(\"accuracy score:\",accuracy_score(y_test,y_pred))\n",
    "  precision, recall, f1_score, _ = precision_recall_fscore_support(y_test,y_pred)\n",
    "  for i, v in enumerate(['p1','p2','p3','p4']):\n",
    "    print(f\"Class{v}\")\n",
    "    print(f\"Precision: {precision[i]}\")\n",
    "    print(f\"recall: {recall[i]}\")\n",
    "    print(f\"F1_score: {f1_score[i]}\")\n",
    "    print()\n",
    "DecisionTree(x_train,y_train,x_test,y_test)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3414866b",
   "metadata": {},
   "source": [
    "Now XGBOOST is the better from other models, now we will go for \n",
    "- Feature engineering\n",
    "- Feature Scaling\n",
    "- Finetune It \n",
    "\n",
    "Why we are doing this?\n",
    "- To increase the accuracy of the model so that it can predict more better"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5c99d8",
   "metadata": {},
   "source": [
    "**To model fintune karne se pahle hamko ye kuch jaruri concepts padh lene chahiye**\n",
    "üìä Confusion Matrix and Classification Metrics \n",
    "\n",
    "---\n",
    "\n",
    "üîπ Confusion Matrix Kya Hota Hai?\n",
    "\n",
    "Confusion Matrix ek 2x2 table hoti hai jisme model ke predicted aur actual results compare kiye jaate hain.\n",
    "\n",
    "|                     | Predicted Positive | Predicted Negative |\n",
    "|---------------------|--------------------|--------------------|\n",
    "| **Actual Positive** | True Positive (TP) | False Negative (FN) |\n",
    "| **Actual Negative** | False Positive (FP) | True Negative (TN) |\n",
    "\n",
    "---\n",
    "\n",
    "üî∏ Meaning of Each Term\n",
    "\n",
    "- **True Positive (TP):** Model ne positive bola aur actual bhi positive tha.  \n",
    "- **True Negative (TN):** Model ne negative bola aur actual bhi negative tha.  \n",
    "- **False Positive (FP):** Model ne positive bola lekin actual negative tha.  \n",
    "- **False Negative (FN):** Model ne negative bola lekin actual positive tha.\n",
    "\n",
    "---\n",
    "\n",
    "üßÆ Formulae\n",
    "\n",
    "- **Accuracy** = (TP + TN) / (TP + TN + FP + FN)  \n",
    "- **Precision** = TP / (TP + FP)  \n",
    "- **Recall (Sensitivity)** = TP / (TP + FN)  \n",
    "- **F1-Score** = 2 * (Precision * Recall) / (Precision + Recall)\n",
    "\n",
    "---\n",
    "\n",
    "1Ô∏è‚É£ Accuracy Example ‚Äì Balanced Dataset\n",
    "\n",
    "**Scenario:** Cat vs Dog classifier (balanced data)\n",
    "\n",
    "| Actual \\ Predicted | Dog | Cat |\n",
    "|---------------------|-----|-----|\n",
    "| Dog | 45 (TP) | 5 (FN) |\n",
    "| Cat | 5 (FP) | 45 (TN) |\n",
    "\n",
    "**Calculations:**\n",
    "- TP = 45  \n",
    "- TN = 45  \n",
    "- FP = 5  \n",
    "- FN = 5  \n",
    "\n",
    "**Accuracy = (TP + TN) / (TP + TN + FP + FN)**  \n",
    "= (45 + 45) / (45 + 45 + 5 + 5)  \n",
    "= 90 / 100  \n",
    "= **0.9 or 90%**\n",
    "\n",
    "‚úÖ **Use Accuracy when dataset is balanced.**  \n",
    "‚ùå Not good for imbalanced data.\n",
    "\n",
    "---\n",
    "\n",
    "2Ô∏è‚É£ Precision Example ‚Äì When False Positives Are Costly\n",
    "\n",
    "**Scenario:** Spam detection system  \n",
    "(Important emails galti se spam na ho jayein ‚Üí FP ko kam rakhna zaroori)\n",
    "\n",
    "| Actual \\ Predicted | Spam | Not Spam |\n",
    "|---------------------|-------|----------|\n",
    "| Spam | 30 (TP) | 10 (FN) |\n",
    "| Not Spam | 40 (FP) | 120 (TN) |\n",
    "\n",
    "**Calculations:**\n",
    "- TP = 30  \n",
    "- FP = 40  \n",
    "- FN = 10  \n",
    "- TN = 120  \n",
    "\n",
    "**Precision = TP / (TP + FP)**  \n",
    "= 30 / (30 + 40)  \n",
    "= 30 / 70  \n",
    "= **0.43 or 43%**\n",
    "\n",
    "**Recall = TP / (TP + FN)**  \n",
    "= 30 / (30 + 10)  \n",
    "= 30 / 40  \n",
    "= **0.75 or 75%**\n",
    "\n",
    "üîπ Model ne 43% cases me sahi spam bola (low precision).  \n",
    "üîπ Lekin 75% actual spam detect kar liye (high recall).\n",
    "\n",
    "‚úÖ Use **Precision** jab **false positives avoid** karne ho (FP costly).  \n",
    "üìçExample: Important mail ko galti se spam na mark karna.\n",
    "\n",
    "---\n",
    "\n",
    "3Ô∏è‚É£ Recall Example ‚Äì When False Negatives Are Costly\n",
    "\n",
    "**Scenario:** Cancer detection  \n",
    "(Actual cancer patient ko miss karna ‚Äì FN ‚Äì dangerous hai)\n",
    "\n",
    "| Actual \\ Predicted | Cancer | No Cancer |\n",
    "|---------------------|---------|-----------|\n",
    "| Cancer | 90 (TP) | 10 (FN) |\n",
    "| No Cancer | 30 (FP) | 870 (TN) |\n",
    "\n",
    "**Calculations:**\n",
    "- TP = 90  \n",
    "- FN = 10  \n",
    "- FP = 30  \n",
    "- TN = 870  \n",
    "\n",
    "**Recall = TP / (TP + FN)**  \n",
    "= 90 / (90 + 10)  \n",
    "= 90 / 100  \n",
    "= **0.9 or 90%**\n",
    "\n",
    "**Precision = TP / (TP + FP)**  \n",
    "= 90 / (90 + 30)  \n",
    "= 90 / 120  \n",
    "= **0.75 or 75%**\n",
    "\n",
    "üîπ Model ne 90% actual cancer cases sahi detect kiye (high recall).  \n",
    "üîπ Lekin kuch false alarms (FP) aaye ‚Äî precision thoda low.\n",
    "\n",
    "‚úÖ Use **Recall** jab **false negatives dangerous ho**.  \n",
    "üìçExample: Disease detection, fraud detection.\n",
    "\n",
    "---\n",
    "\n",
    "4Ô∏è‚É£ F1-Score Example ‚Äì Balance Between Precision and Recall\n",
    "\n",
    "**Scenario:** Fraud detection  \n",
    "(Dono ‚Äî galat fraud bolna aur actual fraud miss karna ‚Äî dono costly hain)\n",
    "\n",
    "| Actual \\ Predicted | Fraud | Not Fraud |\n",
    "|---------------------|--------|------------|\n",
    "| Fraud | 70 (TP) | 30 (FN) |\n",
    "| Not Fraud | 40 (FP) | 860 (TN) |\n",
    "\n",
    "**Calculations:**\n",
    "- TP = 70  \n",
    "- FP = 40  \n",
    "- FN = 30  \n",
    "- TN = 860  \n",
    "\n",
    "**Precision = TP / (TP + FP)**  \n",
    "= 70 / (70 + 40)  \n",
    "= 70 / 110  \n",
    "= **0.64 or 64%**\n",
    "\n",
    "**Recall = TP / (TP + FN)**  \n",
    "= 70 / (70 + 30)  \n",
    "= 70 / 100  \n",
    "= **0.7 or 70%**\n",
    "\n",
    "**F1-Score = 2 * (Precision * Recall) / (Precision + Recall)**  \n",
    "= 2 * (0.64 * 0.7) / (0.64 + 0.7)  \n",
    "= 2 * (0.448) / 1.34  \n",
    "= 0.896 / 1.34  \n",
    "= **0.67 or 67%**\n",
    "\n",
    "üîπ F1-score dono ka balance dikhata hai ‚Äî Precision aur Recall dono moderate hain.  \n",
    "‚úÖ Use F1 jab dono important ho aur data imbalanced ho.  \n",
    "üìçExample: Fraud detection, sentiment analysis.\n",
    "\n",
    "---\n",
    "\n",
    "üìà Summary Table\n",
    "\n",
    "| Metric | Formula | Focus | Best Use Case |\n",
    "|---------|----------|--------|---------------|\n",
    "| **Accuracy** | (TP+TN)/(TP+TN+FP+FN) | Overall correctness | Balanced dataset |\n",
    "| **Precision** | TP/(TP+FP) | True positives out of predicted positives | FP costly ho |\n",
    "| **Recall** | TP/(TP+FN) | True positives out of actual positives | FN costly ho |\n",
    "| **F1-score** | 2 * (P*R)/(P+R) | Balance between Precision & Recall | Jab dono important ho |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6097123",
   "metadata": {},
   "source": [
    "Note: **Very Important**\n",
    "- By looking at the imbalanced nature of the target varibale, we get to decide the loss metric "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e713ad3e",
   "metadata": {},
   "source": [
    "üìä Regression Evaluation Metrics\n",
    "\n",
    "Regression metrics ka kaam hai ye **measure karna ki model predictions kitni accurate hain**.  \n",
    "Alag-alag metrics ka **focus alag hota hai**, aur har metric ka **pros-cons aur limitations** hai.  \n",
    "\n",
    "---\n",
    "\n",
    "üîπ Sample Data\n",
    "\n",
    "- Actual (y_true) = [100, 200, 300, 400, 500]  \n",
    "- Predicted (y_pred) = [110, 190, 290, 410, 480]  \n",
    "\n",
    "Errors = y_true - y_pred = [-10, 10, 10, -10, 20]\n",
    "\n",
    "---\n",
    "\n",
    "1Ô∏è‚É£ MAE (Mean Absolute Error)\n",
    "\n",
    "**Formula:**  \n",
    "MAE = Average(|y_true - y_pred|)\n",
    "\n",
    "**Calculation:**  \n",
    "|Error|10|10|10|10|20| ‚Üí Sum = 60  \n",
    "MAE = 60 / 5 = **12**\n",
    "\n",
    "**Pros:**\n",
    "- Easy to interpret (average error directly samajh aata hai).  \n",
    "- Outliers ka impact MSE/RMSE ke comparison me kam.  \n",
    "\n",
    "**Cons:**\n",
    "- Large errors ko zyada penalize nahi karta.  \n",
    "- Gradient-based optimization me thoda tricky ho sakta hai (non-differentiable at 0).  \n",
    "\n",
    "**Use Case ‚úÖ:**  \n",
    "- Jab average error ko direct samajhna ho (e.g., house price prediction, simple forecasting).  \n",
    "\n",
    "**Avoid ‚ùå:**  \n",
    "- Jab large errors ko zyada punish karna ho (e.g., critical safety predictions).\n",
    "\n",
    "---\n",
    "\n",
    "2Ô∏è‚É£ MSE (Mean Squared Error)\n",
    "\n",
    "**Formula:**  \n",
    "MSE = Average((y_true - y_pred)¬≤)\n",
    "\n",
    "**Calculation:**  \n",
    "Errors¬≤ = [100, 100, 100, 100, 400] ‚Üí Sum = 800  \n",
    "MSE = 800 / 5 = **160**\n",
    "\n",
    "**Pros:**\n",
    "- Large errors ko zyada punish karta hai (squared error).  \n",
    "- Optimization ke liye differentiable ‚Üí commonly used as loss function in regression.  \n",
    "\n",
    "**Cons:**\n",
    "- Units squared ho jaate hain ‚Üí interpret karna mushkil.  \n",
    "- Outliers bahut effect karte hain.  \n",
    "\n",
    "**Use Case ‚úÖ:**  \n",
    "- Jab large errors critical ho aur model ko strict train karna ho.  \n",
    "\n",
    "**Avoid ‚ùå:**  \n",
    "- Jab interpretability chahiye aur outliers ho.  \n",
    "\n",
    "**Example:**  \n",
    "Stock price prediction, energy consumption modeling.\n",
    "\n",
    "---\n",
    "\n",
    "3Ô∏è‚É£ RMSE (Root Mean Squared Error)\n",
    "\n",
    "**Formula:**  \n",
    "RMSE = sqrt(MSE)\n",
    "\n",
    "**Calculation:**  \n",
    "RMSE = sqrt(160) ‚âà **12.65**\n",
    "\n",
    "**Pros:**\n",
    "- Same as MSE, lekin units original scale me (interpret karna easy).  \n",
    "- Large errors ko punish karta hai.  \n",
    "\n",
    "**Cons:**\n",
    "- Outliers se sensitive hai. \n",
    "- Scale dependent hai **example**\n",
    "  - Age k case mien baat karien to agar **RMSE = 20** aa raha hai to ye gadbad hai \n",
    "  - No of cells k case mien agar **RMSE = 20** aa raha to model bohot acha perform kar raha hai kyunki cells to crores mien hotien hain  \n",
    "\n",
    "**Use Case ‚úÖ:**  \n",
    "- Jab units maintain karna ho aur large error penalize karna ho.  \n",
    "\n",
    "**Avoid ‚ùå:**  \n",
    "- Agar dataset me extreme outliers ho ‚Üí misleading ho sakta hai.\n",
    "\n",
    "**Example:**  \n",
    "Weather forecasting (temperature deviation me large errors costly).\n",
    "\n",
    "---\n",
    "\n",
    "4Ô∏è‚É£ R¬≤ Score (Coefficient of Determination)\n",
    "\n",
    "**Formula:**  \n",
    "R¬≤ = 1 - (Sum of Squared Errors / Total Variance)\n",
    "\n",
    "**Calculation (approx):**  \n",
    "- Mean(y_true) = 300  \n",
    "- SST = sum((y_true - mean)¬≤) = 100000  \n",
    "- SSE = sum((y_true - y_pred)¬≤) = 800  \n",
    "- R¬≤ = 1 - 800 / 100000 = 0.992\n",
    "\n",
    "**Pros:**\n",
    "- Model ka **overall fit** measure karta hai (0-1 scale).  \n",
    "- Easy to compare models.\n",
    "\n",
    "**Cons:**\n",
    "- Non-linear models me misleading ho sakta hai.  \n",
    "- Outliers se effect hota hai.  \n",
    "\n",
    "**Use Case ‚úÖ:**  \n",
    "- Linear regression me model fit evaluate karna.  \n",
    "- Compare multiple models on same data.  \n",
    "\n",
    "**Avoid ‚ùå:**  \n",
    "- Non-linear ya tree-based models me R¬≤ kabhi misleading ho sakta hai.  \n",
    "\n",
    "**Example:**  \n",
    "House price prediction model ‚Äî R¬≤ = 0.99 ‚Üí almost perfect fit.\n",
    "\n",
    "---\n",
    "\n",
    "5Ô∏è‚É£ MAPE (Mean Absolute Percentage Error)\n",
    "\n",
    "**Formula:**  \n",
    "MAPE = Average(|(y_true - y_pred)/y_true| * 100)\n",
    "\n",
    "**Calculation:**  \n",
    "Errors % = [10%, 5%, 3.33%, 2.5%, 4%] ‚Üí Sum = 24.83%  \n",
    "MAPE = 24.83 / 5 ‚âà **4.97%**\n",
    "\n",
    "**Pros:**\n",
    "- Percentage me error bataata hai ‚Üí easy to explain.  \n",
    "- Units se independent.\n",
    "- Scale independent hota hai ye\n",
    "\n",
    "**Cons:**\n",
    "- Zero or negative y_true me error divide by zero problem.  \n",
    "- Outliers ka zyada impact ho sakta hai.\n",
    "\n",
    "**Use Case ‚úÖ:**  \n",
    "- Business metrics me forecasting (sales, revenue).  \n",
    "\n",
    "**Avoid ‚ùå:**  \n",
    "- Zero or negative actual values wale dataset me.  \n",
    "\n",
    "**Example:**  \n",
    "Sales forecast ‚Äî model 5% average error se predict karta hai.\n",
    "\n",
    "---\n",
    "\n",
    "6Ô∏è‚É£ Comparative Table (All Metrics)\n",
    "\n",
    "| Metric | Focus | Pros | Cons | Use Case | Avoid |\n",
    "|--------|-------|------|------|----------|-------|\n",
    "| MAE | Average absolute error | Easy, interpretable | Large errors not penalized | House price | Critical large errors |\n",
    "| MSE | Squared error | Penalizes large errors, good for optimization | Units squared, sensitive to outliers | Energy, stock prices | Outliers, interpretability |\n",
    "| RMSE | Root of MSE | Same as MSE, units original | Sensitive to outliers | Weather, critical prediction | Outliers |\n",
    "| R¬≤ | Model fit (0-1) | Overall fit, compare models | Misleading for non-linear/outlier-heavy data | Linear regression | Non-linear/tree models |\n",
    "| MAPE | Percentage error | Easy to explain, unit-independent | Zero/negative actuals, outliers | Business/sales forecast | Zero/negative targets |\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2c80f2",
   "metadata": {},
   "source": [
    "Ab hamm Hyperparameter tuning karenge\n",
    "- Hyperparameter matlab bohot saare alag alag parameters(variables) ki bohot saari values lenge jo inbuilt **model function** k andar hotien hain \n",
    "```python \n",
    "XGBClassifier(objective='multi:softmax',num_class=4) \n",
    "# jaise ismien ye do variables hain objective, num_classes aise har parameter ki bohot saari values lete hain or jo combination of values sabse badiya accuracy deta hai train or test data dono pe usko final kar dete hain\n",
    "```\n",
    "**Jaise pahle radios hote the to unmien alag alag frequencies hoti thi jis frequency pe channel pakad jata vo best frequency**\n",
    "(**usi ko hamm tuning kahte hain**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "18adbe56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning \n",
    "\n",
    "# Defining the hyperparameters grid \n",
    "parameterGrid = {\n",
    "  'colsample_bytree': [0.1,], #0.3,0.5,0.7,0.9\n",
    "  'learning_rate': [0.001,], #0.01,0.1,1\n",
    "  'max_depth': [3,10], #5,8,\n",
    "  'alpha':[1,10,], #100\n",
    "  'n_estimators':[10,50,] #100\n",
    "}\n",
    "\n",
    "index = 0\n",
    "\n",
    "answers_grid = {\n",
    "  'combination' : [],\n",
    "  'train_Accuracy': [],\n",
    "  'test_Accuracy': [],\n",
    "  'colsample_bytree': [],\n",
    "  'learning_rate': [],\n",
    "  'max_depth': [],\n",
    "  'alpha':[],\n",
    "  'n_estimators':[]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6bc51e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combination 1\n",
      "colsample_bytree: 0.1, learning_rate : 0.001, max_depth : 3, alpha :1, n_estimators :10\n",
      "Train Accuracy : 0.62\n",
      "Test Accuracy : 0.62\n",
      "______________________________\n",
      "Combination 2\n",
      "colsample_bytree: 0.1, learning_rate : 0.001, max_depth : 3, alpha :1, n_estimators :50\n",
      "Train Accuracy : 0.61\n",
      "Test Accuracy : 0.60\n",
      "______________________________\n",
      "Combination 3\n",
      "colsample_bytree: 0.1, learning_rate : 0.001, max_depth : 3, alpha :10, n_estimators :10\n",
      "Train Accuracy : 0.62\n",
      "Test Accuracy : 0.62\n",
      "______________________________\n",
      "Combination 4\n",
      "colsample_bytree: 0.1, learning_rate : 0.001, max_depth : 3, alpha :10, n_estimators :50\n",
      "Train Accuracy : 0.61\n",
      "Test Accuracy : 0.60\n",
      "______________________________\n",
      "Combination 5\n",
      "colsample_bytree: 0.1, learning_rate : 0.001, max_depth : 10, alpha :1, n_estimators :10\n",
      "Train Accuracy : 0.63\n",
      "Test Accuracy : 0.62\n",
      "______________________________\n",
      "Combination 6\n",
      "colsample_bytree: 0.1, learning_rate : 0.001, max_depth : 10, alpha :1, n_estimators :50\n",
      "Train Accuracy : 0.61\n",
      "Test Accuracy : 0.60\n",
      "______________________________\n",
      "Combination 7\n",
      "colsample_bytree: 0.1, learning_rate : 0.001, max_depth : 10, alpha :10, n_estimators :10\n",
      "Train Accuracy : 0.63\n",
      "Test Accuracy : 0.62\n",
      "______________________________\n",
      "Combination 8\n",
      "colsample_bytree: 0.1, learning_rate : 0.001, max_depth : 10, alpha :10, n_estimators :50\n",
      "Train Accuracy : 0.61\n",
      "Test Accuracy : 0.60\n",
      "______________________________\n"
     ]
    }
   ],
   "source": [
    "# Loop through each combination of hyperparameters\n",
    "index = 0\n",
    "for colsample_bytree in parameterGrid['colsample_bytree']:\n",
    "  for learning_rate in parameterGrid['learning_rate']:\n",
    "    for max_depth in parameterGrid['max_depth']:\n",
    "      for alpha in parameterGrid['alpha']:\n",
    "        for n_estimators in parameterGrid['n_estimators']: \n",
    "\n",
    "          index = index+1 \n",
    "\n",
    "          # Define and train the XGBOOST model \n",
    "          model = xgb.XGBClassifier(objective='mutli:softmax',\n",
    "                                    num_class=4,\n",
    "                                    colsample_bytree = colsample_bytree,\n",
    "                                    learning_rate = learning_rate,\n",
    "                                    max_depth = max_depth,\n",
    "                                    n_estimators = n_estimators)\n",
    "          \n",
    "          model.fit(x_train,y_train)\n",
    "\n",
    "          # predict on training and testing sets \n",
    "          y_pred_train = model.predict(x_train)\n",
    "          y_pred_test = model.predict(x_test)\n",
    "\n",
    "          # Calculating accuracy scores\n",
    "          train_accuracy = accuracy_score(y_train,y_pred_train)\n",
    "          test_accuracy = accuracy_score(y_test,y_pred_test)\n",
    "\n",
    "          # Include into the lists of the answer grid dictionay\n",
    "          answers_grid['combination'].append(index)\n",
    "          answers_grid['train_Accuracy'].append(train_accuracy)\n",
    "          answers_grid['test_Accuracy'].append(test_accuracy)\n",
    "          answers_grid['colsample_bytree'].append(colsample_bytree)\n",
    "          answers_grid['learning_rate'].append(learning_rate)\n",
    "          answers_grid['max_depth'].append(max_depth)\n",
    "          answers_grid['alpha'].append(alpha)\n",
    "          answers_grid['n_estimators'].append(n_estimators)\n",
    "\n",
    "          #print the results of each combination \n",
    "          print(f\"Combination {index}\")\n",
    "          print(f\"colsample_bytree: {colsample_bytree}, learning_rate : {learning_rate}, max_depth : {max_depth}, alpha :{alpha}, n_estimators :{n_estimators}\")\n",
    "          print(f\"Train Accuracy : {train_accuracy:.2f}\")\n",
    "          print(f\"Test Accuracy : {test_accuracy:.2f}\")\n",
    "          print(\"_\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dc930521",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(answers_grid)\n",
    "best_values = results.loc[(results['test_Accuracy']>=0.7) & (results['learning_rate'] < 1) & (results['alpha']==10)].max()\n",
    "# to fir yahan se apne ko best parameters mil jayenge to vo saare parameters values jaake upar vaale XGBOOST model mien daal do jahan pe hamne model train kiya "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6cd1ccb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(\"/home/ankitmaan/Credit Risk Modelling/Datasets/Unseen_Dataset.xlsx\")\n",
    "# One hot and label encoding of unseen data also to find out whether we are getting good results or bad results on unseen data\n",
    "data.loc[data['EDUCATION']=='SSC',['EDUCATION']]=1\n",
    "data.loc[data['EDUCATION']=='12TH',['EDUCATION']]=2\n",
    "data.loc[data['EDUCATION']=='OTHERS',['EDUCATION']]=1\n",
    "data.loc[data['EDUCATION']=='GRADUATE',['EDUCATION']]=3\n",
    "data.loc[data['EDUCATION']=='UNDER GRADUATE',['EDUCATION']]=3\n",
    "data.loc[data['EDUCATION']=='POST-GRADUATE',['EDUCATION']]=4\n",
    "data.loc[data['EDUCATION']=='PROFESSIONAL',['EDUCATION']]=3\n",
    "\n",
    "data['EDUCATION'] = data['EDUCATION'].astype(int)\n",
    "data = pd.get_dummies(data, columns=['MARITALSTATUS','GENDER','last_prod_enq2','first_prod_enq2'],dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "731c3699",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pct_tl_open_L6M</th>\n",
       "      <th>pct_tl_closed_L6M</th>\n",
       "      <th>Tot_TL_closed_L12M</th>\n",
       "      <th>pct_tl_closed_L12M</th>\n",
       "      <th>Tot_Missed_Pmnt</th>\n",
       "      <th>CC_TL</th>\n",
       "      <th>Home_TL</th>\n",
       "      <th>PL_TL</th>\n",
       "      <th>Secured_TL</th>\n",
       "      <th>Unsecured_TL</th>\n",
       "      <th>...</th>\n",
       "      <th>last_prod_enq2_HL</th>\n",
       "      <th>last_prod_enq2_PL</th>\n",
       "      <th>last_prod_enq2_others</th>\n",
       "      <th>first_prod_enq2_AL</th>\n",
       "      <th>first_prod_enq2_CC</th>\n",
       "      <th>first_prod_enq2_ConsumerLoan</th>\n",
       "      <th>first_prod_enq2_HL</th>\n",
       "      <th>first_prod_enq2_PL</th>\n",
       "      <th>first_prod_enq2_others</th>\n",
       "      <th>Approved_Flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows √ó 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    pct_tl_open_L6M  pct_tl_closed_L6M  Tot_TL_closed_L12M  \\\n",
       "0             0.000                0.0                   0   \n",
       "1             0.000                0.0                   0   \n",
       "2             0.125                0.0                   0   \n",
       "3             0.000                0.0                   0   \n",
       "4             0.000                0.0                   1   \n",
       "..              ...                ...                 ...   \n",
       "95            0.000                0.0                   0   \n",
       "96            0.222                0.0                   2   \n",
       "97            0.000                0.0                   0   \n",
       "98            0.000                1.0                   1   \n",
       "99            0.500                0.0                   0   \n",
       "\n",
       "    pct_tl_closed_L12M  Tot_Missed_Pmnt  CC_TL  Home_TL  PL_TL  Secured_TL  \\\n",
       "0                0.000                0      0        0      4           1   \n",
       "1                0.000                0      0        0      0           0   \n",
       "2                0.000                1      0        0      0           2   \n",
       "3                0.000                0      0        0      0           3   \n",
       "4                0.167                0      0        0      0           6   \n",
       "..                 ...              ...    ...      ...    ...         ...   \n",
       "95               0.000                0      0        0      1           1   \n",
       "96               0.222                0      0        0      0           1   \n",
       "97               0.000                0      0        0      0           1   \n",
       "98               1.000                0      0        0      0           1   \n",
       "99               0.000                0      0        0      0           1   \n",
       "\n",
       "    Unsecured_TL  ...  last_prod_enq2_HL  last_prod_enq2_PL  \\\n",
       "0              4  ...                  0                  1   \n",
       "1              1  ...                  0                  0   \n",
       "2              6  ...                  0                  0   \n",
       "3              0  ...                  0                  0   \n",
       "4              0  ...                  0                  0   \n",
       "..           ...  ...                ...                ...   \n",
       "95             5  ...                  0                  0   \n",
       "96             8  ...                  0                  0   \n",
       "97             0  ...                  0                  0   \n",
       "98             0  ...                  0                  0   \n",
       "99             1  ...                  0                  0   \n",
       "\n",
       "    last_prod_enq2_others  first_prod_enq2_AL  first_prod_enq2_CC  \\\n",
       "0                       0                   0                   0   \n",
       "1                       0                   0                   0   \n",
       "2                       0                   0                   0   \n",
       "3                       0                   1                   0   \n",
       "4                       0                   0                   0   \n",
       "..                    ...                 ...                 ...   \n",
       "95                      0                   0                   0   \n",
       "96                      0                   0                   0   \n",
       "97                      1                   0                   0   \n",
       "98                      1                   0                   0   \n",
       "99                      1                   0                   0   \n",
       "\n",
       "    first_prod_enq2_ConsumerLoan  first_prod_enq2_HL  first_prod_enq2_PL  \\\n",
       "0                              0                   0                   1   \n",
       "1                              1                   0                   0   \n",
       "2                              0                   0                   0   \n",
       "3                              0                   0                   0   \n",
       "4                              0                   0                   1   \n",
       "..                           ...                 ...                 ...   \n",
       "95                             0                   0                   0   \n",
       "96                             0                   0                   0   \n",
       "97                             0                   0                   0   \n",
       "98                             0                   0                   0   \n",
       "99                             0                   0                   0   \n",
       "\n",
       "    first_prod_enq2_others  Approved_Flag  \n",
       "0                        0              1  \n",
       "1                        0              1  \n",
       "2                        1              1  \n",
       "3                        0              1  \n",
       "4                        0              1  \n",
       "..                     ...            ...  \n",
       "95                       1              1  \n",
       "96                       1              1  \n",
       "97                       1              1  \n",
       "98                       1              1  \n",
       "99                       1              1  \n",
       "\n",
       "[100 rows x 55 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = pd.Series(model.predict(data))\n",
    "prediction.value_counts()  # It is performing somewhat nearby to original data if we compare value counts \n",
    "data['Approved_Flag']=prediction\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "00726d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pct_tl_open_L6M       0.0\n",
      "pct_tl_closed_L6M     0.0\n",
      "Tot_TL_closed_L12M    0.0\n",
      "pct_tl_closed_L12M    0.0\n",
      "Tot_Missed_Pmnt       0.0\n",
      "CC_TL                 0.0\n",
      "Home_TL               0.0\n",
      "PL_TL                 0.0\n",
      "Secured_TL            0.0\n",
      "Unsecured_TL          1.0\n",
      "Name: 1, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(data.iloc[1,:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
